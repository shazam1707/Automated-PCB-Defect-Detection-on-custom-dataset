{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13071081,"sourceType":"datasetVersion","datasetId":8278064},{"sourceId":13076314,"sourceType":"datasetVersion","datasetId":8281661}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dependencies","metadata":{}},{"cell_type":"code","source":"import IPython.display as display\n!pip uninstall -y requests\n\n!pip install -U urllib3 requests\ndisplay.clear_output()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y torch torchvision\n!pip install torch==2.0.0 torchvision==0.15.1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U openmim\n!pip install \"mmengine>=0.7.1,<1.0.0\" \\\n \"mmcv>=2.0.0rc4,<2.1.0\" \\\n -f https://download.openmmlab.com/mmcv/dist/cu117/torch2.0.0/index.html \\\n--trusted-host download.openmmlab.com","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install mmdetection\n!rm -rf mmdetection\n!git clone https://github.com/open-mmlab/mmdetection.git\n%cd mmdetection\n\n%pip install -e .","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import Dataset","metadata":{}},{"cell_type":"code","source":"#copying the dataset to mmyolo directory\n\nimport shutil\nimport os\n\n# Source directory to copy\nsrc_dir = '/kaggle/input/loserspcb-v2/combined_trialv4_updated'\n\n# Destination directory where the source directory will be copied\ndst_dir = '/kaggle/working/mmdetection/datasets'\n\n# Remove destination directory if it exists\nif os.path.exists(dst_dir):\n    shutil.rmtree(dst_dir)\n\n# Copy the entire directory tree\nshutil.copytree(src_dir, dst_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Config modification ","metadata":{}},{"cell_type":"code","source":"config_coco = \"\"\"\n\n\ndataset_type = 'CocoDataset'\ndata_root = './datasets/'\n\n# Example to use different file client\n# Method 1: simply set the data root and let the file I/O module\n# automatically infer from prefix (not support LMDB and Memcache yet)\n\n# data_root = 's3://openmmlab/datasets/detection/coco/'\n\ntrain_batch_size_per_gpu = 1\nval_batch_size_per_gpu = 1\ntrain_num_workers = 1 \nval_num_workers = 1 \nmetainfo = {\n    'classes': ('MP','OC','SC','SP','SPC')\n    }\n\n# Method 2: Use `backend_args`, `file_client_args` in versions before 3.0.0rc6\n# backend_args = dict(\n#     backend='petrel',\n#     path_mapping=dict({\n#         './data/': 's3://openmmlab/datasets/detection/',\n#         'data/': 's3://openmmlab/datasets/detection/'\n#     }))\nbackend_args = None\n\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', backend_args=backend_args),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', scale=(960,960), keep_ratio=True),\n    dict(type='PackDetInputs')\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile', backend_args=backend_args),\n    dict(type='Resize', scale=(960,960), keep_ratio=True),\n    # If you don't have a gt annotation, delete the pipeline\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='PackDetInputs',\n        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n                   'scale_factor'))\n]\ntrain_dataloader = dict(\n    batch_size=train_batch_size_per_gpu,\n    num_workers=train_num_workers,\n    persistent_workers=True,\n    sampler=dict(type='DefaultSampler', shuffle=True),\n    batch_sampler=dict(type='AspectRatioBatchSampler'),\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        metainfo=metainfo,\n        ann_file='train.json',\n        data_prefix=dict(img='train/'),\n        filter_cfg=dict(filter_empty_gt=True, min_size=32),\n        pipeline=train_pipeline,\n        backend_args=backend_args))\nval_dataloader = dict(\n    batch_size=val_batch_size_per_gpu,\n    num_workers=val_num_workers,\n    persistent_workers=True,\n    drop_last=False,\n    sampler=dict(type='DefaultSampler', shuffle=False),\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        metainfo=metainfo,\n        ann_file='val.json',\n        data_prefix=dict(img='val/'),\n        test_mode=True,\n        pipeline=test_pipeline,\n        backend_args=backend_args))\ntest_dataloader = val_dataloader\n\nval_evaluator = dict(\n    type='CocoMetric',\n    ann_file=data_root + 'val.json',\n    metric='bbox',\n    classwise=True,\n    format_only=False,\n    backend_args=backend_args)\ntest_evaluator = val_evaluator\n\n\n\n\"\"\"\nwith open('./configs/_base_/datasets/coco_detection.py', 'w') as f:\n    f.write(config_coco)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#config for augmentation\n#/kaggle/working/mmdetection/mmdet/configs/common/ms_3x_coco.py\n#config for model\n#/kaggle/working/mmdetection/projects/CO-DETR/configs/codino/co_dino_5scale_r50_lsj_8xb2_1x_coco.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#/kaggle/working/mmdetection/configs/_base_/datasets/coco_detection.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#for visualising the config\nfrom mmengine import Config\nimport json\ncfg = Config.fromfile('/kaggle/working/mmdetection/configs/_base_/datasets/coco_detection.py')\nformatted_cfg = json.dumps(cfg._cfg_dict, indent=4)\n\nprint(formatted_cfg)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config_pcb_defect = \"\"\"\n\n_base_ = [\n    '../_base_/default_runtime.py', '../_base_/schedules/schedule_2x.py',\n    '../_base_/datasets/coco_detection.py'\n]\n\ncustom_imports = dict(\n    imports=['projects.CO-DETR.codetr'], allow_failed_imports=False)\n\n# model settings\nnum_dec_layer = 6\nloss_lambda = 2.0\nnum_classes = 5\n\n\nmodel = dict(\n    type='CoDETR',\n    # If using the lsj augmentation,\n    # it is recommended to set it to True.\n    use_lsj=False, #Not using any lsj based augmentation \n    # detr: 52.1\n    # one-stage: 49.4\n    # two-stage: 47.9\n    eval_module='detr',  # in ['detr', 'one-stage', 'two-stage']\n    data_preprocessor=dict(\n        type='DetDataPreprocessor',\n        mean=[123.675, 116.28, 103.53],\n        std=[58.395, 57.12, 57.375],\n        bgr_to_rgb=True,\n        pad_mask=False,\n        batch_augments=None),\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        norm_eval=True,\n        style='pytorch',\n        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),\n    neck=dict(\n        type='ChannelMapper',\n        in_channels=[256, 512, 1024, 2048],\n        kernel_size=1,\n        out_channels=256,\n        act_cfg=None,\n        norm_cfg=dict(type='GN', num_groups=32),\n        num_outs=5),\n    query_head=dict(\n        type='CoDINOHead',\n        num_query=900,\n        num_classes=num_classes,\n        in_channels=2048,\n        as_two_stage=True,\n        dn_cfg=dict(\n            label_noise_scale=0.5,\n            box_noise_scale=1.0,\n            group_cfg=dict(dynamic=True, num_groups=None, num_dn_queries=100)),\n        transformer=dict(\n            type='CoDinoTransformer',\n            with_coord_feat=False,\n            num_co_heads=2,  # ATSS Aux Head + Faster RCNN Aux Head\n            num_feature_levels=5,\n            encoder=dict(\n                type='DetrTransformerEncoder',\n                num_layers=6,\n                # number of layers that use checkpoint.\n                # The maximum value for the setting is num_layers.\n                # FairScale must be installed for it to work.\n                with_cp=4,\n                transformerlayers=dict(\n                    type='BaseTransformerLayer',\n                    attn_cfgs=dict(\n                        type='MultiScaleDeformableAttention',\n                        embed_dims=256,\n                        num_levels=5,\n                        dropout=0.0),\n                    feedforward_channels=2048,\n                    ffn_dropout=0.0,\n                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),\n            decoder=dict(\n                type='DinoTransformerDecoder',\n                num_layers=6,\n                return_intermediate=True,\n                transformerlayers=dict(\n                    type='DetrTransformerDecoderLayer',\n                    attn_cfgs=[\n                        dict(\n                            type='MultiheadAttention',\n                            embed_dims=256,\n                            num_heads=8,\n                            dropout=0.0),\n                        dict(\n                            type='MultiScaleDeformableAttention',\n                            embed_dims=256,\n                            num_levels=5,\n                            dropout=0.0),\n                    ],\n                    feedforward_channels=2048,\n                    ffn_dropout=0.0,\n                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',\n                                     'ffn', 'norm')))),\n        positional_encoding=dict(\n            type='SinePositionalEncoding',\n            num_feats=128,\n            temperature=20,\n            normalize=True),\n        loss_cls=dict(  # Different from the DINO\n            type='QualityFocalLoss',\n            use_sigmoid=True,\n            beta=2.0,\n            loss_weight=1.0),\n        loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n        loss_iou=dict(type='GIoULoss', loss_weight=2.0)),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_generator=dict(\n            type='AnchorGenerator',\n            octave_base_scale=4,\n            scales_per_octave=3,\n            ratios=[0.5, 1.0, 2.0],\n            strides=[4, 8, 16, 32, 64, 128]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        loss_cls=dict(\n            type='CrossEntropyLoss',\n            use_sigmoid=True,\n            loss_weight=1.0 * num_dec_layer * loss_lambda),\n        loss_bbox=dict(\n            type='L1Loss', loss_weight=1.0 * num_dec_layer * loss_lambda)),\n    roi_head=[\n        dict(\n            type='CoStandardRoIHead',\n            bbox_roi_extractor=dict(\n                type='SingleRoIExtractor',\n                roi_layer=dict(\n                    type='RoIAlign', output_size=7, sampling_ratio=0),\n                out_channels=256,\n                featmap_strides=[4, 8, 16, 32, 64],\n                finest_scale=56),\n            bbox_head=dict(\n                type='Shared2FCBBoxHead',\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                num_classes=num_classes,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.1, 0.1, 0.2, 0.2]),\n                reg_class_agnostic=False,\n                reg_decoded_bbox=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0 * num_dec_layer * loss_lambda),\n                loss_bbox=dict(\n                    type='GIoULoss',\n                    loss_weight=10.0 * num_dec_layer * loss_lambda)))\n    ],\n    bbox_head=[\n        dict(\n            type='CoATSSHead',\n            num_classes=num_classes,\n            in_channels=256,\n            stacked_convs=1,\n            feat_channels=256,\n            anchor_generator=dict(\n                type='AnchorGenerator',\n                ratios=[1.0],\n                octave_base_scale=8,\n                scales_per_octave=1,\n                strides=[4, 8, 16, 32, 64, 128]),\n            bbox_coder=dict(\n                type='DeltaXYWHBBoxCoder',\n                target_means=[.0, .0, .0, .0],\n                target_stds=[0.1, 0.1, 0.2, 0.2]),\n            loss_cls=dict(\n                type='FocalLoss',\n                use_sigmoid=True,\n                gamma=2.0,\n                alpha=0.25,\n                loss_weight=1.0 * num_dec_layer * loss_lambda),\n            loss_bbox=dict(\n                type='GIoULoss',\n                loss_weight=2.0 * num_dec_layer * loss_lambda),\n            loss_centerness=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=True,\n                loss_weight=1.0 * num_dec_layer * loss_lambda)),\n    ],\n    # model training and testing settings\n    train_cfg=[\n        dict(\n            assigner=dict(\n                type='HungarianAssigner',\n                match_costs=[\n                    dict(type='FocalLossCost', weight=2.0),\n                    dict(type='BBoxL1Cost', weight=5.0, box_format='xywh'),\n                    dict(type='IoUCost', iou_mode='giou', weight=2.0)\n                ])),\n        dict(\n            rpn=dict(\n                assigner=dict(\n                    type='MaxIoUAssigner',\n                    pos_iou_thr=0.7,\n                    neg_iou_thr=0.3,\n                    min_pos_iou=0.3,\n                    match_low_quality=True,\n                    ignore_iof_thr=-1),\n                sampler=dict(\n                    type='RandomSampler',\n                    num=256,\n                    pos_fraction=0.5,\n                    neg_pos_ub=-1,\n                    add_gt_as_proposals=False),\n                allowed_border=-1,\n                pos_weight=-1,\n                debug=False),\n            rpn_proposal=dict(\n                nms_pre=4000,\n                max_per_img=1000,\n                nms=dict(type='nms', iou_threshold=0.7),\n                min_bbox_size=0),\n            rcnn=dict(\n                assigner=dict(\n                    type='MaxIoUAssigner',\n                    pos_iou_thr=0.5,\n                    neg_iou_thr=0.5,\n                    min_pos_iou=0.5,\n                    match_low_quality=False,\n                    ignore_iof_thr=-1),\n                sampler=dict(\n                    type='RandomSampler',\n                    num=512,\n                    pos_fraction=0.25,\n                    neg_pos_ub=-1,\n                    add_gt_as_proposals=True),\n                pos_weight=-1,\n                debug=False)),\n        dict(\n            assigner=dict(type='ATSSAssigner', topk=9),\n            allowed_border=-1,\n            pos_weight=-1,\n            debug=False)\n    ],\n    test_cfg=[\n        # Deferent from the DINO, we use the NMS.\n        dict(\n            max_per_img=300,\n            # NMS can improve the mAP by 0.2.\n            nms=dict(type='soft_nms', iou_threshold=0.8)),\n        dict(\n            rpn=dict(\n                nms_pre=1000,\n                max_per_img=1000,\n                nms=dict(type='nms', iou_threshold=0.7),\n                min_bbox_size=0),\n            rcnn=dict(\n                score_thr=0.0,\n                nms=dict(type='nms', iou_threshold=0.5),\n                max_per_img=100)),\n        dict(\n            # atss bbox head:\n            nms_pre=1000,\n            min_bbox_size=0,\n            score_thr=0.0,\n            nms=dict(type='nms', iou_threshold=0.6),\n            max_per_img=100),\n        # soft-nms is also supported for rcnn testing\n        # e.g., nms=dict(type='soft_nms', iou_threshold=0.5, min_score=0.05)\n    ])\n\nmax_epochs = 12\n# training schedule for 2x\ntrain_cfg = dict(\n    _delete_=True,\n    type='EpochBasedTrainLoop',\n    max_epochs=max_epochs,\n    val_interval=1)\n    \n\noptim_wrapper = dict(\n    _delete_=True,\n    type='OptimWrapper',\n    optimizer=dict(type='AdamW', lr=2e-4, weight_decay=0.0001),\n    clip_grad=dict(max_norm=0.1, norm_type=2),\n    paramwise_cfg=dict(custom_keys={'backbone': dict(lr_mult=0.1)}))\n\nval_evaluator = dict(metric='bbox')\ntest_evaluator = val_evaluator\n\nparam_scheduler = [\n    dict(\n        type='MultiStepLR',\n        begin=0,\n        end=max_epochs,\n        by_epoch=True,\n        milestones=[11],\n        gamma=0.1)\n]\n\ndefault_hooks = dict(\n    checkpoint=dict(by_epoch=True, interval=1, max_keep_ckpts=1,save_best='auto'))\nlog_processor = dict(by_epoch=True)\n\n\n# NOTE: `auto_scale_lr` is for automatically scaling LR,\n# USER SHOULD NOT CHANGE ITS VALUES.\n# base_batch_size = (8 GPUs) x (2 samples per GPU)\nauto_scale_lr = dict(base_batch_size=2)\n\n\"\"\"\nwith open('/kaggle/working/mmdetection/configs/detr/co_dino_5scale_r50_lsj_8xb2_1x_pcb_defect.py', 'w') as f:\n    f.write(config_pcb_defect)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#load_from = '/kaggle/input/weights/co_dino_5scale_r50_lsj_8xb2_1x_coco-69a72d67.pth'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#for visualising the config\nfrom mmengine import Config\nimport json\ncfg = Config.fromfile('/kaggle/working/mmdetection/configs/detr/co_dino_5scale_r50_lsj_8xb2_1x_pcb_defect.py')\nformatted_cfg = json.dumps(cfg._cfg_dict, indent=4)\n\nprint(formatted_cfg)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# saving the config file\nfrom mmengine import Config\nimport json\n\n# Load the configuration from file\ncfg = Config.fromfile('/kaggle/working/mmdetection/configs/detr/co_dino_5scale_r50_lsj_8xb2_1x_pcb_defect.py')\n\n# Convert the configuration to a dictionary and then to a formatted JSON string\nformatted_cfg = json.dumps(cfg._cfg_dict, indent=4)\n\n# Define the output file path\noutput_file = '/kaggle/working/formatted_config.json'\n\n# Save the formatted JSON string to a file\nwith open(output_file, 'w') as f:\n    f.write(formatted_cfg)\n\nprint(f'Configuration saved as {output_file}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualizing the Dataset ","metadata":{}},{"cell_type":"code","source":"\n# # Basically, as the browse_dataset code of mmdet does not work\n# # i had to copy the browse_dataset code of mmyolo here, one line had to change\n# # from mmyolo.registry import DATASETS, VISUALIZERS >>> from mmdet.registry import DATASETS, VISUALIZERS\n\n# code ='''\n# # Copyright (c) OpenMMLab. All rights reserved.\n# import argparse\n# import os.path as osp\n# import sys\n# from typing import Tuple\n\n# import cv2\n# import mmcv\n# import numpy as np\n# from mmdet.models.utils import mask2ndarray\n# from mmdet.structures.bbox import BaseBoxes\n# from mmengine.config import Config, DictAction\n# from mmengine.dataset import Compose\n# from mmengine.registry import init_default_scope\n# from mmengine.utils import ProgressBar\n# from mmengine.visualization import Visualizer\n\n# from mmdet.registry import DATASETS, VISUALIZERS\n\n\n# # TODO: Support for printing the change in key of results\n# # TODO: Some bug. If you meet some bug, please use the original\n# def parse_args():\n#     parser = argparse.ArgumentParser(description='Browse a dataset')\n#     parser.add_argument('config', help='train config file path')\n#     parser.add_argument(\n#         '--phase',\n#         '-p',\n#         default='train',\n#         type=str,\n#         choices=['train', 'test', 'val'],\n#         help='phase of dataset to visualize, accept \"train\" \"test\" and \"val\".'\n#         ' Defaults to \"train\".')\n#     parser.add_argument(\n#         '--mode',\n#         '-m',\n#         default='transformed',\n#         type=str,\n#         choices=['original', 'transformed', 'pipeline'],\n#         help='display mode; display original pictures or '\n#         'transformed pictures or comparison pictures. \"original\" '\n#         'means show images load from disk; \"transformed\" means '\n#         'to show images after transformed; \"pipeline\" means show all '\n#         'the intermediate images. Defaults to \"transformed\".')\n#     parser.add_argument(\n#         '--out-dir',\n#         default='output',\n#         type=str,\n#         help='If there is no display interface, you can save it.')\n#     parser.add_argument('--not-show', default=False, action='store_true')\n#     parser.add_argument(\n#         '--show-number',\n#         '-n',\n#         type=int,\n#         default=sys.maxsize,\n#         help='number of images selected to visualize, '\n#         'must bigger than 0. if the number is bigger than length '\n#         'of dataset, show all the images in dataset; '\n#         'default \"sys.maxsize\", show all images in dataset')\n#     parser.add_argument(\n#         '--show-interval',\n#         '-i',\n#         type=float,\n#         default=3,\n#         help='the interval of show (s)')\n#     parser.add_argument(\n#         '--cfg-options',\n#         nargs='+',\n#         action=DictAction,\n#         help='override some settings in the used config, the key-value pair '\n#         'in xxx=yyy format will be merged into config file. If the value to '\n#         'be overwritten is a list, it should be like key=\"[a,b]\" or key=a,b '\n#         'It also allows nested list/tuple values, e.g. key=\"[(a,b),(c,d)]\" '\n#         'Note that the quotation marks are necessary and that no white space '\n#         'is allowed.')\n#     args = parser.parse_args()\n#     return args\n\n\n# def _get_adaptive_scale(img_shape: Tuple[int, int],\n#                         min_scale: float = 0.3,\n#                         max_scale: float = 3.0) -> float:\n#     \"\"\"Get adaptive scale according to image shape.\n\n#     The target scale depends on the the short edge length of the image. If the\n#     short edge length equals 224, the output is 1.0. And output linear\n#     scales according the short edge length. You can also specify the minimum\n#     scale and the maximum scale to limit the linear scale.\n\n#     Args:\n#         img_shape (Tuple[int, int]): The shape of the canvas image.\n#         min_scale (int): The minimum scale. Defaults to 0.3.\n#         max_scale (int): The maximum scale. Defaults to 3.0.\n#     Returns:\n#         int: The adaptive scale.\n#     \"\"\"\n#     short_edge_length = min(img_shape)\n#     scale = short_edge_length / 224.\n#     return min(max(scale, min_scale), max_scale)\n\n\n# def make_grid(imgs, names):\n#     \"\"\"Concat list of pictures into a single big picture, align height here.\"\"\"\n#     visualizer = Visualizer.get_current_instance()\n#     ori_shapes = [img.shape[:2] for img in imgs]\n#     max_height = int(max(img.shape[0] for img in imgs) * 1.1)\n#     min_width = min(img.shape[1] for img in imgs)\n#     horizontal_gap = min_width // 10\n#     img_scale = _get_adaptive_scale((max_height, min_width))\n\n#     texts = []\n#     text_positions = []\n#     start_x = 0\n#     for i, img in enumerate(imgs):\n#         pad_height = (max_height - img.shape[0]) // 2\n#         pad_width = horizontal_gap // 2\n#         # make border\n#         imgs[i] = cv2.copyMakeBorder(\n#             img,\n#             pad_height,\n#             max_height - img.shape[0] - pad_height + int(img_scale * 30 * 2),\n#             pad_width,\n#             pad_width,\n#             cv2.BORDER_CONSTANT,\n#             value=(255, 255, 255))\n#         texts.append(f'{\"execution: \"}{i}\\\\n{names[i]}\\\\n{ori_shapes[i]}')\n\n#         text_positions.append(\n#             [start_x + img.shape[1] // 2 + pad_width, max_height])\n#         start_x += img.shape[1] + horizontal_gap\n\n#     display_img = np.concatenate(imgs, axis=1)\n#     visualizer.set_image(display_img)\n#     img_scale = _get_adaptive_scale(display_img.shape[:2])\n#     visualizer.draw_texts(\n#         texts,\n#         positions=np.array(text_positions),\n#         font_sizes=img_scale * 7,\n#         colors='black',\n#         horizontal_alignments='center',\n#         font_families='monospace')\n#     return visualizer.get_image()\n\n\n# def swap_pipeline_position(dataset_cfg):\n#     load_ann_tfm_name = 'LoadAnnotations'\n#     pipeline = dataset_cfg.get('pipeline')\n#     if (pipeline is None):\n#         return dataset_cfg\n#     all_transform_types = [tfm['type'] for tfm in pipeline]\n#     if load_ann_tfm_name in all_transform_types:\n#         load_ann_tfm_index = all_transform_types.index(load_ann_tfm_name)\n#         load_ann_tfm = pipeline.pop(load_ann_tfm_index)\n#         pipeline.insert(1, load_ann_tfm)\n\n\n# class InspectCompose(Compose):\n#     \"\"\"Compose multiple transforms sequentially.\n\n#     And record \"img\" field of all results in one list.\n#     \"\"\"\n\n#     def __init__(self, transforms, intermediate_imgs):\n#         super().__init__(transforms=transforms)\n#         self.intermediate_imgs = intermediate_imgs\n\n#     def __call__(self, data):\n#         if 'img' in data:\n#             self.intermediate_imgs.append({\n#                 'name': 'original',\n#                 'img': data['img'].copy()\n#             })\n#         self.ptransforms = [\n#             self.transforms[i] for i in range(len(self.transforms) - 1)\n#         ]\n#         for t in self.ptransforms:\n#             data = t(data)\n#             # Keep the same meta_keys in the PackDetInputs\n#             self.transforms[-1].meta_keys = [key for key in data]\n#             data_sample = self.transforms[-1](data)\n#             if data is None:\n#                 return None\n#             if 'img' in data:\n#                 self.intermediate_imgs.append({\n#                     'name':\n#                     t.__class__.__name__,\n#                     'dataset_sample':\n#                     data_sample['data_samples']\n#                 })\n#         return data\n\n\n# def main():\n#     args = parse_args()\n#     cfg = Config.fromfile(args.config)\n#     if args.cfg_options is not None:\n#         cfg.merge_from_dict(args.cfg_options)\n\n#     init_default_scope(cfg.get('default_scope', 'mmyolo'))\n\n#     dataset_cfg = cfg.get(args.phase + '_dataloader').get('dataset')\n#     if (args.phase in ['test', 'val']):\n#         swap_pipeline_position(dataset_cfg)\n#     dataset = DATASETS.build(dataset_cfg)\n#     visualizer = VISUALIZERS.build(cfg.visualizer)\n#     visualizer.dataset_meta = dataset.metainfo\n\n#     intermediate_imgs = []\n\n#     if not hasattr(dataset, 'pipeline'):\n#         # for dataset_wrapper\n#         dataset = dataset.dataset\n\n#     # TODO: The dataset wrapper occasion is not considered here\n#     dataset.pipeline = InspectCompose(dataset.pipeline.transforms,\n#                                       intermediate_imgs)\n\n#     # init visualization image number\n#     assert args.show_number > 0\n#     display_number = min(args.show_number, len(dataset))\n\n#     progress_bar = ProgressBar(display_number)\n#     for i, item in zip(range(display_number), dataset):\n#         image_i = []\n#         result_i = [result['dataset_sample'] for result in intermediate_imgs]\n#         for k, datasample in enumerate(result_i):\n#             image = datasample.img\n#             gt_instances = datasample.gt_instances\n#             image = image[..., [2, 1, 0]]  # bgr to rgb\n#             gt_bboxes = gt_instances.get('bboxes', None)\n#             if gt_bboxes is not None and isinstance(gt_bboxes, BaseBoxes):\n#                 gt_instances.bboxes = gt_bboxes.tensor\n#             gt_masks = gt_instances.get('masks', None)\n#             if gt_masks is not None:\n#                 masks = mask2ndarray(gt_masks)\n#                 gt_instances.masks = masks.astype(bool)\n#                 datasample.gt_instances = gt_instances\n#             # get filename from dataset or just use index as filename\n#             visualizer.add_datasample(\n#                 'result',\n#                 image,\n#                 datasample,\n#                 draw_pred=False,\n#                 draw_gt=True,\n#                 show=False)\n#             image_show = visualizer.get_image()\n#             image_i.append(image_show)\n\n#         if args.mode == 'original':\n#             image = image_i[0]\n#         elif args.mode == 'transformed':\n#             image = image_i[-1]\n#         else:\n#             image = make_grid([result for result in image_i],\n#                               [result['name'] for result in intermediate_imgs])\n\n#         if hasattr(datasample, 'img_path'):\n#             filename = osp.basename(datasample.img_path)\n#         else:\n#             # some dataset have not image path\n#             filename = f'{i}.jpg'\n#         out_file = osp.join(args.out_dir,\n#                             filename) if args.out_dir is not None else None\n\n#         if out_file is not None:\n#             mmcv.imwrite(image[..., ::-1], out_file)\n\n#         if not args.not_show:\n#             visualizer.show(\n#                 image, win_name=filename, wait_time=args.show_interval)\n\n#         intermediate_imgs.clear()\n#         progress_bar.update()\n\n\n# if __name__ == '__main__':\n#     main()\n# '''\n\n# with open('./tools/analysis_tools/browse_dataset_2.py', 'w') as f:\n#     f.write(code)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python tools/analysis_tools/browse_dataset_2.py configs/detr/co_dino_5scale_r50_lsj_8xb2_1x_pcb_defect.py --mode pipeline --out-dir dataset_check_6 --show-number 6 --show-interval 2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"#!python tools/train.py configs/detr/co_dino_5scale_r50_lsj_8xb2_1x_pcb_defect.py --work-dir co-detr_dspcbsd_12e/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!bash ./tools/dist_train.sh configs/detr/co_dino_5scale_r50_lsj_8xb2_1x_pcb_defect.py 2 --work-dir co-detr_loserspcb_12e/ ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Just to look at the converted data\n#Saving the weight files\nimport shutil\n\n# Directory to be zipped\ndirectory_to_zip = '/kaggle/working/mmdetection/co-detr_loserspcb_12e'\n\n# Destination zip file path\nzip_file_path = '/kaggle/working/co-detr_loserspcb_12e.zip'\n\n# Create a zip file\nshutil.make_archive(zip_file_path[:-4], 'zip', directory_to_zip)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing ","metadata":{}},{"cell_type":"code","source":"# !python tools/test.py \\\n# configs/rtmdet/rtmdet_m_8xb32-50e_pcb_defect.py \\\n# /kaggle/input/inference-2/best_coco_SH_precision_epoch_45.pth\\\n# --work-dir results/evaluate \\\n# --out results/results.pkl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Flops","metadata":{}},{"cell_type":"code","source":"# #I modified the get_flops scripts so that it works\n\n# import shutil\n\n# #Define the source file path and the destination file path\n# source = '/kaggle/input/get-flops-update/get_flops_2.py'\n# destination = '/kaggle/working/mmdetection/tools/analysis_tools/get_flops_2.py'\n\n# #Copy the file\n# shutil.copy(source, destination)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python tools/analysis_tools/get_flops_2.py configs/rtmdet/rtmdet_m_8xb32-50e_pcb_defect.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Benchmark","metadata":{}},{"cell_type":"code","source":"# !python tools/analysis_tools/benchmark.py configs/rtmdet/rtmdet_m_8xb32-50e_pcb_defect.py \\\n# --checkpoint /kaggle/input/inference-3/best_coco_bbox_mAP_epoch_45.pth \\\n# --task inference \\\n# --repeat-num 5 \\\n# --max-iter 100 \\\n# --log-interval 50 \\\n# --num-warmup 10 \\\n# --work-dir ./results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Confusion Matrix","metadata":{}},{"cell_type":"code","source":"\n# import shutil\n\n# # Define the source file path and the destination file path\n# source = '/kaggle/input/confusion-matrix-mod/confusion_matrix.py'\n# destination = '/kaggle/working/mmdetection/tools/analysis_tools/confusion_matrix_4.py'\n\n# # Copy the file\n# shutil.copy(source, destination)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python tools/analysis_tools/confusion_matrix_4.py \\\n# configs/rtmdet/rtmdet_m_8xb32-50e_pcb_defect.py  \\\n# results/results.pkl  \\\n# ./results \\\n# --show \\\n# --score-thr 0.5 \\\n# --tp-iou-thr 0.5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Saving the results","metadata":{}},{"cell_type":"code","source":"# import shutil\n\n# # Directory to be zipped\n# directory_to_zip = '/kaggle/working/mmdetection/results'\n\n# # Destination zip file path\n# zip_file_path = '/kaggle/working/rtmdet-m_dspcbsd_results.zip'\n\n# # Create a zip file\n# shutil.make_archive(zip_file_path[:-4], 'zip', directory_to_zip)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# from mmdet.apis import DetInferencer\n# import glob\n\n# # Choose to use a config\n# config = 'configs/rtmdet/rtmdet_m_8xb32-50e_pcb_defect.py'\n# # Setup a checkpoint file to load\n# checkpoint = '/kaggle/input/inference-2/best_coco_SH_precision_epoch_45.pth'\n\n# # Set the device to be used for evaluation\n# device = 'cuda:0'\n\n# # Initialize the DetInferencer\n# inferencer = DetInferencer(config, checkpoint, device)\n\n# # Use the detector to do inference\n# img = '/kaggle/input/dspcbsd/DsPCBSD+/Data_COCO/val/0373736.jpg'\n\n# result = inferencer(img, out_dir='./output',no_save_pred=False, pred_score_thr=0.5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Show the output image\n# from PIL import Image\n# Image.open('./output/vis/0373736.jpg')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}