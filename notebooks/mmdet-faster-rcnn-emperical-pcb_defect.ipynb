{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13080214,"sourceType":"datasetVersion","datasetId":8284419}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dependencies","metadata":{}},{"cell_type":"code","source":"import IPython.display as display\n!pip uninstall -y requests\n\n!pip install -U urllib3 requests\ndisplay.clear_output()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y torch torchvision\n!pip install torch==2.0.0 torchvision==0.15.1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U openmim\n!pip install \"mmengine>=0.7.1,<1.0.0\" \\\n \"mmcv>=2.0.0rc4,<2.1.0\" \\\n -f https://download.openmmlab.com/mmcv/dist/cu117/torch2.0.0/index.html \\\n--trusted-host download.openmmlab.com","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install mmdetection\n!rm -rf mmdetection\n!git clone https://github.com/open-mmlab/mmdetection.git\n%cd mmdetection\n\n%pip install -e .","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#copying the dataset to mmyolo directory\n\nimport shutil\nimport os\n\n# Source directory to copy\nsrc_dir = '/kaggle/input/loserspcb-v2/combined_trialv4_updated'\n\n# Destination directory where the source directory will be copied\ndst_dir = '/kaggle/working/mmdetection/datasets'\n\n# Remove destination directory if it exists\nif os.path.exists(dst_dir):\n    shutil.rmtree(dst_dir)\n\n# Copy the entire directory tree\nshutil.copytree(src_dir, dst_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Config modification ","metadata":{}},{"cell_type":"code","source":"config_coco = \"\"\"\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = './datasets/'\n\ntrain_batch_size_per_gpu = 4\nval_batch_size_per_gpu = 2\ntrain_num_workers = 1 \nval_num_workers = 1 \nmetainfo = {\n    'classes': ('MP','OC','SC','SP','SPC')\n    }\nbackend_args = None\n\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', backend_args=backend_args),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', scale=(1024, 1024), keep_ratio=True),\n    dict(type='PackDetInputs')\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile', backend_args=backend_args),\n    dict(type='Resize', scale=(1024, 1024), keep_ratio=True),\n    # If you don't have a gt annotation, delete the pipeline\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='PackDetInputs',\n        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n                   'scale_factor'))\n]\ntrain_dataloader = dict(\n    batch_size=train_batch_size_per_gpu,\n    num_workers=train_num_workers,\n    persistent_workers=True,\n    sampler=dict(type='DefaultSampler', shuffle=True),\n    batch_sampler=dict(type='AspectRatioBatchSampler'),\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        metainfo=metainfo,\n        ann_file='train.json',\n        data_prefix=dict(img='train/'),\n        filter_cfg=dict(filter_empty_gt=True, min_size=32),\n        pipeline=train_pipeline,\n        backend_args=backend_args))\nval_dataloader = dict(\n    batch_size=val_batch_size_per_gpu,\n    num_workers=val_num_workers,\n    persistent_workers=True,\n    drop_last=False,\n    sampler=dict(type='DefaultSampler', shuffle=False),\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        metainfo=metainfo,\n        ann_file='val.json',\n        data_prefix=dict(img='val/'),\n        test_mode=True,\n        pipeline=test_pipeline,\n        backend_args=backend_args))\ntest_dataloader = val_dataloader\n\nval_evaluator = dict(\n    type='CocoMetric',\n    ann_file=data_root + 'val.json',\n    classwise=True,\n    metric='bbox',\n    format_only=False,\n    backend_args=backend_args)\ntest_evaluator = val_evaluator\n\n\"\"\"\nwith open('./configs/_base_/datasets/coco_detection.py', 'w') as f:\n    f.write(config_coco)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#for visualising the config\nfrom mmengine import Config\nimport json\ncfg = Config.fromfile('/kaggle/working/mmdetection/configs/_base_/datasets/coco_detection.py')\nformatted_cfg = json.dumps(cfg._cfg_dict, indent=4)\n\nprint(formatted_cfg)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config_pcb_defect = \"\"\"\n\n_base_ = '../faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        plugins=[\n            dict(\n                cfg=dict(\n                    type='GeneralizedAttention',\n                    spatial_range=-1,\n                    num_heads=8,\n                    attention_type='0010',\n                    kv_stride=2),\n                stages=(False, False, True, True),\n                position='after_conv2')\n        ],\n        dcn=dict(type='DCN', deform_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n\nmax_epochs = 50\n\ntrain_cfg = dict(\n    type='EpochBasedTrainLoop',\n    max_epochs=max_epochs,\n    val_interval=2)\n\n# hooks\ndefault_hooks = dict(\n    checkpoint=dict(\n        type='CheckpointHook',\n        interval=5,\n        max_keep_ckpts=1,\n        save_best = 'auto'),\n        logger=dict(type='LoggerHook', interval=5))     # only keep latest 1 checkpoints\n\n\"\"\"\nwith open('./configs/empirical_attention/faster-rcnn_r50-attn0010-dcn_fpn_1x_pcb_defect.py', 'w') as f:\n    f.write(config_pcb_defect)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#for visualising the config\nfrom mmengine import Config\nimport json\ncfg = Config.fromfile('configs/empirical_attention/faster-rcnn_r50-attn0010-dcn_fpn_1x_pcb_defect.py')\nformatted_cfg = json.dumps(cfg._cfg_dict, indent=4)\n\nprint(formatted_cfg)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# saving the config file\nfrom mmengine import Config\nimport json\n\n# Load the configuration from file\ncfg = Config.fromfile('configs/empirical_attention/faster-rcnn_r50-attn0010-dcn_fpn_1x_pcb_defect.py')\n\n# Convert the configuration to a dictionary and then to a formatted JSON string\nformatted_cfg = json.dumps(cfg._cfg_dict, indent=4)\n\n# Define the output file path\noutput_file = '/kaggle/working/formatted_config.json'\n\n# Save the formatted JSON string to a file\nwith open(output_file, 'w') as f:\n    f.write(formatted_cfg)\n\nprint(f'Configuration saved as {output_file}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualization ","metadata":{}},{"cell_type":"code","source":"\n# # Basically, as the browse_dataset code of mmdet does not work\n# # i had to copy the browse_dataset code of mmyolo here, one line had to change\n# # from mmyolo.registry import DATASETS, VISUALIZERS >>> from mmdet.registry import DATASETS, VISUALIZERS\n\n# code ='''\n# # Copyright (c) OpenMMLab. All rights reserved.\n# import argparse\n# import os.path as osp\n# import sys\n# from typing import Tuple\n\n# import cv2\n# import mmcv\n# import numpy as np\n# from mmdet.models.utils import mask2ndarray\n# from mmdet.structures.bbox import BaseBoxes\n# from mmengine.config import Config, DictAction\n# from mmengine.dataset import Compose\n# from mmengine.registry import init_default_scope\n# from mmengine.utils import ProgressBar\n# from mmengine.visualization import Visualizer\n\n# from mmdet.registry import DATASETS, VISUALIZERS\n\n\n# # TODO: Support for printing the change in key of results\n# # TODO: Some bug. If you meet some bug, please use the original\n# def parse_args():\n#     parser = argparse.ArgumentParser(description='Browse a dataset')\n#     parser.add_argument('config', help='train config file path')\n#     parser.add_argument(\n#         '--phase',\n#         '-p',\n#         default='train',\n#         type=str,\n#         choices=['train', 'test', 'val'],\n#         help='phase of dataset to visualize, accept \"train\" \"test\" and \"val\".'\n#         ' Defaults to \"train\".')\n#     parser.add_argument(\n#         '--mode',\n#         '-m',\n#         default='transformed',\n#         type=str,\n#         choices=['original', 'transformed', 'pipeline'],\n#         help='display mode; display original pictures or '\n#         'transformed pictures or comparison pictures. \"original\" '\n#         'means show images load from disk; \"transformed\" means '\n#         'to show images after transformed; \"pipeline\" means show all '\n#         'the intermediate images. Defaults to \"transformed\".')\n#     parser.add_argument(\n#         '--out-dir',\n#         default='output',\n#         type=str,\n#         help='If there is no display interface, you can save it.')\n#     parser.add_argument('--not-show', default=False, action='store_true')\n#     parser.add_argument(\n#         '--show-number',\n#         '-n',\n#         type=int,\n#         default=sys.maxsize,\n#         help='number of images selected to visualize, '\n#         'must bigger than 0. if the number is bigger than length '\n#         'of dataset, show all the images in dataset; '\n#         'default \"sys.maxsize\", show all images in dataset')\n#     parser.add_argument(\n#         '--show-interval',\n#         '-i',\n#         type=float,\n#         default=3,\n#         help='the interval of show (s)')\n#     parser.add_argument(\n#         '--cfg-options',\n#         nargs='+',\n#         action=DictAction,\n#         help='override some settings in the used config, the key-value pair '\n#         'in xxx=yyy format will be merged into config file. If the value to '\n#         'be overwritten is a list, it should be like key=\"[a,b]\" or key=a,b '\n#         'It also allows nested list/tuple values, e.g. key=\"[(a,b),(c,d)]\" '\n#         'Note that the quotation marks are necessary and that no white space '\n#         'is allowed.')\n#     args = parser.parse_args()\n#     return args\n\n\n# def _get_adaptive_scale(img_shape: Tuple[int, int],\n#                         min_scale: float = 0.3,\n#                         max_scale: float = 3.0) -> float:\n#     \"\"\"Get adaptive scale according to image shape.\n\n#     The target scale depends on the the short edge length of the image. If the\n#     short edge length equals 224, the output is 1.0. And output linear\n#     scales according the short edge length. You can also specify the minimum\n#     scale and the maximum scale to limit the linear scale.\n\n#     Args:\n#         img_shape (Tuple[int, int]): The shape of the canvas image.\n#         min_scale (int): The minimum scale. Defaults to 0.3.\n#         max_scale (int): The maximum scale. Defaults to 3.0.\n#     Returns:\n#         int: The adaptive scale.\n#     \"\"\"\n#     short_edge_length = min(img_shape)\n#     scale = short_edge_length / 224.\n#     return min(max(scale, min_scale), max_scale)\n\n\n# def make_grid(imgs, names):\n#     \"\"\"Concat list of pictures into a single big picture, align height here.\"\"\"\n#     visualizer = Visualizer.get_current_instance()\n#     ori_shapes = [img.shape[:2] for img in imgs]\n#     max_height = int(max(img.shape[0] for img in imgs) * 1.1)\n#     min_width = min(img.shape[1] for img in imgs)\n#     horizontal_gap = min_width // 10\n#     img_scale = _get_adaptive_scale((max_height, min_width))\n\n#     texts = []\n#     text_positions = []\n#     start_x = 0\n#     for i, img in enumerate(imgs):\n#         pad_height = (max_height - img.shape[0]) // 2\n#         pad_width = horizontal_gap // 2\n#         # make border\n#         imgs[i] = cv2.copyMakeBorder(\n#             img,\n#             pad_height,\n#             max_height - img.shape[0] - pad_height + int(img_scale * 30 * 2),\n#             pad_width,\n#             pad_width,\n#             cv2.BORDER_CONSTANT,\n#             value=(255, 255, 255))\n#         texts.append(f'{\"execution: \"}{i}\\\\n{names[i]}\\\\n{ori_shapes[i]}')\n\n#         text_positions.append(\n#             [start_x + img.shape[1] // 2 + pad_width, max_height])\n#         start_x += img.shape[1] + horizontal_gap\n\n#     display_img = np.concatenate(imgs, axis=1)\n#     visualizer.set_image(display_img)\n#     img_scale = _get_adaptive_scale(display_img.shape[:2])\n#     visualizer.draw_texts(\n#         texts,\n#         positions=np.array(text_positions),\n#         font_sizes=img_scale * 7,\n#         colors='black',\n#         horizontal_alignments='center',\n#         font_families='monospace')\n#     return visualizer.get_image()\n\n\n# def swap_pipeline_position(dataset_cfg):\n#     load_ann_tfm_name = 'LoadAnnotations'\n#     pipeline = dataset_cfg.get('pipeline')\n#     if (pipeline is None):\n#         return dataset_cfg\n#     all_transform_types = [tfm['type'] for tfm in pipeline]\n#     if load_ann_tfm_name in all_transform_types:\n#         load_ann_tfm_index = all_transform_types.index(load_ann_tfm_name)\n#         load_ann_tfm = pipeline.pop(load_ann_tfm_index)\n#         pipeline.insert(1, load_ann_tfm)\n\n\n# class InspectCompose(Compose):\n#     \"\"\"Compose multiple transforms sequentially.\n\n#     And record \"img\" field of all results in one list.\n#     \"\"\"\n\n#     def __init__(self, transforms, intermediate_imgs):\n#         super().__init__(transforms=transforms)\n#         self.intermediate_imgs = intermediate_imgs\n\n#     def __call__(self, data):\n#         if 'img' in data:\n#             self.intermediate_imgs.append({\n#                 'name': 'original',\n#                 'img': data['img'].copy()\n#             })\n#         self.ptransforms = [\n#             self.transforms[i] for i in range(len(self.transforms) - 1)\n#         ]\n#         for t in self.ptransforms:\n#             data = t(data)\n#             # Keep the same meta_keys in the PackDetInputs\n#             self.transforms[-1].meta_keys = [key for key in data]\n#             data_sample = self.transforms[-1](data)\n#             if data is None:\n#                 return None\n#             if 'img' in data:\n#                 self.intermediate_imgs.append({\n#                     'name':\n#                     t.__class__.__name__,\n#                     'dataset_sample':\n#                     data_sample['data_samples']\n#                 })\n#         return data\n\n\n# def main():\n#     args = parse_args()\n#     cfg = Config.fromfile(args.config)\n#     if args.cfg_options is not None:\n#         cfg.merge_from_dict(args.cfg_options)\n\n#     init_default_scope(cfg.get('default_scope', 'mmyolo'))\n\n#     dataset_cfg = cfg.get(args.phase + '_dataloader').get('dataset')\n#     if (args.phase in ['test', 'val']):\n#         swap_pipeline_position(dataset_cfg)\n#     dataset = DATASETS.build(dataset_cfg)\n#     visualizer = VISUALIZERS.build(cfg.visualizer)\n#     visualizer.dataset_meta = dataset.metainfo\n\n#     intermediate_imgs = []\n\n#     if not hasattr(dataset, 'pipeline'):\n#         # for dataset_wrapper\n#         dataset = dataset.dataset\n\n#     # TODO: The dataset wrapper occasion is not considered here\n#     dataset.pipeline = InspectCompose(dataset.pipeline.transforms,\n#                                       intermediate_imgs)\n\n#     # init visualization image number\n#     assert args.show_number > 0\n#     display_number = min(args.show_number, len(dataset))\n\n#     progress_bar = ProgressBar(display_number)\n#     for i, item in zip(range(display_number), dataset):\n#         image_i = []\n#         result_i = [result['dataset_sample'] for result in intermediate_imgs]\n#         for k, datasample in enumerate(result_i):\n#             image = datasample.img\n#             gt_instances = datasample.gt_instances\n#             image = image[..., [2, 1, 0]]  # bgr to rgb\n#             gt_bboxes = gt_instances.get('bboxes', None)\n#             if gt_bboxes is not None and isinstance(gt_bboxes, BaseBoxes):\n#                 gt_instances.bboxes = gt_bboxes.tensor\n#             gt_masks = gt_instances.get('masks', None)\n#             if gt_masks is not None:\n#                 masks = mask2ndarray(gt_masks)\n#                 gt_instances.masks = masks.astype(bool)\n#                 datasample.gt_instances = gt_instances\n#             # get filename from dataset or just use index as filename\n#             visualizer.add_datasample(\n#                 'result',\n#                 image,\n#                 datasample,\n#                 draw_pred=False,\n#                 draw_gt=True,\n#                 show=False)\n#             image_show = visualizer.get_image()\n#             image_i.append(image_show)\n\n#         if args.mode == 'original':\n#             image = image_i[0]\n#         elif args.mode == 'transformed':\n#             image = image_i[-1]\n#         else:\n#             image = make_grid([result for result in image_i],\n#                               [result['name'] for result in intermediate_imgs])\n\n#         if hasattr(datasample, 'img_path'):\n#             filename = osp.basename(datasample.img_path)\n#         else:\n#             # some dataset have not image path\n#             filename = f'{i}.jpg'\n#         out_file = osp.join(args.out_dir,\n#                             filename) if args.out_dir is not None else None\n\n#         if out_file is not None:\n#             mmcv.imwrite(image[..., ::-1], out_file)\n\n#         if not args.not_show:\n#             visualizer.show(\n#                 image, win_name=filename, wait_time=args.show_interval)\n\n#         intermediate_imgs.clear()\n#         progress_bar.update()\n\n\n# if __name__ == '__main__':\n#     main()\n# '''\n\n# with open('./tools/analysis_tools/browse_dataset_2.py', 'w') as f:\n#     f.write(code)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python tools/analysis_tools/browse_dataset_2.py configs/empirical_attention/faster-rcnn_r50-attn0010-dcn_fpn_1x_pcb_defect.py --mode pipeline --out-dir dataset_check_3 --show-number 3 --show-interval 12","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training ","metadata":{}},{"cell_type":"code","source":"!bash ./tools/dist_train.sh configs/empirical_attention/faster-rcnn_r50-attn0010-dcn_fpn_1x_pcb_defect.py 2 --work-dir faster-rcnn_loserspcb_50e/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Just to look at the converted data\n\nimport shutil\n\n# Directory to be zipped\ndirectory_to_zip = '/kaggle/working/mmdetection/faster-rcnn_loserspcb_50e'\n\n# Destination zip file path\nzip_file_path = '/kaggle/working/faster-rcnn_loserspcb_50e.zip'\n\n# Create a zip file\nshutil.make_archive(zip_file_path[:-4], 'zip', directory_to_zip)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"# !python tools/test.py \\\n# configs/empirical_attention/faster-rcnn_r50-attn0010-dcn_fpn_1x_pcb_defect.py \\\n# /kaggle/input/inference/best_coco_SH_precision_epoch_30.pth\\\n# --work-dir results/evaluate \\\n# --out results/results.pkl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Flops","metadata":{}},{"cell_type":"code","source":"# #I modified the get_flops scripts so that it works\n\n# import shutil\n\n# #Define the source file path and the destination file path\n# source = '/kaggle/input/get-flops-2/get_flops_2.py'\n# destination = '/kaggle/working/mmdetection/tools/analysis_tools/get_flops_2.py'\n\n# #Copy the file\n# shutil.copy(source, destination)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python tools/analysis_tools/get_flops_2.py configs/empirical_attention/faster-rcnn_r50-attn0010-dcn_fpn_1x_pcb_defect.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Benchmarking","metadata":{}},{"cell_type":"code","source":"# !python tools/analysis_tools/benchmark.py configs/empirical_attention/faster-rcnn_r50-attn0010-dcn_fpn_1x_pcb_defect.py \\\n# --checkpoint /kaggle/input/inference/best_coco_SH_precision_epoch_30.pth \\\n# --task inference \\\n# --repeat-num 5 \\\n# --max-iter 100 \\\n# --log-interval 50 \\\n# --num-warmup 10 \\\n# --work-dir ./results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Confusion Matrix","metadata":{}},{"cell_type":"code","source":"\n# import shutil\n\n# # Define the source file path and the destination file path\n# source = '/kaggle/input/confusion-matrix/confusion_matrix.py'\n# destination = '/kaggle/working/mmdetection/tools/analysis_tools/confusion_matrix_4.py'\n\n# # Copy the file\n# shutil.copy(source, destination)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python tools/analysis_tools/confusion_matrix_4.py \\\n# configs/empirical_attention/faster-rcnn_r50-attn0010-dcn_fpn_1x_pcb_defect.py  \\\n# results/results.pkl  \\\n# ./results \\\n# --show \\\n# --score-thr 0.5 \\\n# --tp-iou-thr 0.5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Results saving","metadata":{}},{"cell_type":"code","source":"# import shutil\n\n# # Directory to be zipped\n# directory_to_zip = '/kaggle/working/mmdetection/results'\n\n# # Destination zip file path\n# zip_file_path = '/kaggle/working/faster-ecnn_dspcbsd_results.zip'\n\n# # Create a zip file\n# shutil.make_archive(zip_file_path[:-4], 'zip', directory_to_zip)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference ","metadata":{}},{"cell_type":"code","source":"# from mmdet.apis import DetInferencer\n# import glob\n\n# # Choose to use a config\n# config = 'configs/empirical_attention/faster-rcnn_r50-attn0010-dcn_fpn_1x_pcb_defect.py'\n# # Setup a checkpoint file to load\n# checkpoint = '/kaggle/input/inference/best_coco_SH_precision_epoch_30.pth'\n\n# # Set the device to be used for evaluation\n# device = 'cuda:0'\n\n# # Initialize the DetInferencer\n# inferencer = DetInferencer(config, checkpoint, device)\n\n# # Use the detector to do inference\n# img = '/kaggle/input/dspcbsd-coco/Data_COCO/val/0045409.jpg'\n\n# result = inferencer(img, out_dir='./output',no_save_pred=False, pred_score_thr=0.5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Show the output image\n# from PIL import Image\n# Image.open('./output/vis/0045409.jpg')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}